#!/usr/bin/env node

import path from "path";
import fs from "fs";
import { runBatchTests } from "../src/test-rewards-integration";



/**
 * Batch test runner script
 * Runs all scenario files in the test-rewards directory
 */

async function main() {
  console.log("ðŸš€ Starting batch test execution...");

  try {
    // Find all scenario files
    const scenarioFiles = findScenarioFiles();
    
    if (scenarioFiles.length === 0) {
      console.log("No scenario files found in test-rewards directory");
      process.exit(0);
    }

    console.log(`Found ${scenarioFiles.length} scenario files:`);
    scenarioFiles.forEach((file, index) => {
      console.log(`  ${index + 1}. ${path.basename(file)}`);
    });

    // Run batch tests
    const reports = await runBatchTests(scenarioFiles);

    // Generate summary report
    generateSummaryReport(reports);

    // Exit with appropriate code
    const hasFailures = reports.some(report => !report.success);
    process.exit(hasFailures ? 1 : 0);

  } catch (error) {
    console.error("âŒ Batch test execution failed:", error);
    process.exit(1);
  }
}

function findScenarioFiles(): string[] {
  const testRewardsDir = path.join(__dirname, "..");
  const scenarioFiles: string[] = [];

  // Look for scenario files in the main directory
  const files = fs.readdirSync(testRewardsDir);
  
  for (const file of files) {
    if (file.startsWith("rewards-scenario-") && file.endsWith(".json")) {
      scenarioFiles.push(path.join(testRewardsDir, file));
    }
  }

  // Also check if there are scenario files generated by the Python script
  const generatedFiles = [
    "rewards-scenario-1.json",
    "rewards-scenario-2.json"
  ];

  for (const file of generatedFiles) {
    const filePath = path.join(testRewardsDir, file);
    if (fs.existsSync(filePath) && !scenarioFiles.includes(filePath)) {
      scenarioFiles.push(filePath);
    }
  }

  return scenarioFiles.sort();
}

function generateSummaryReport(reports: any[]): void {
  console.log(`\n${"=".repeat(100)}`);
  console.log("BATCH TEST EXECUTION SUMMARY");
  console.log(`${"=".repeat(100)}`);

  const totalTests = reports.length;
  const successfulTests = reports.filter(r => r.success).length;
  const failedTests = totalTests - successfulTests;

  // Overall statistics
  console.log(`ðŸ“Š Overall Statistics:`);
  console.log(`   Total Tests: ${totalTests}`);
  console.log(`   Successful: ${successfulTests} (${((successfulTests / totalTests) * 100).toFixed(1)}%)`);
  console.log(`   Failed: ${failedTests} (${((failedTests / totalTests) * 100).toFixed(1)}%)`);

  // Execution time statistics
  const executionTimes = reports.map(r => r.executionTime);
  const totalTime = executionTimes.reduce((sum, time) => sum + time, 0);
  const avgTime = totalTime / executionTimes.length;
  const minTime = Math.min(...executionTimes);
  const maxTime = Math.max(...executionTimes);

  console.log(`\nâ±ï¸  Execution Time Statistics:`);
  console.log(`   Total: ${totalTime.toFixed(1)}s`);
  console.log(`   Average: ${avgTime.toFixed(1)}s`);
  console.log(`   Min: ${minTime.toFixed(1)}s`);
  console.log(`   Max: ${maxTime.toFixed(1)}s`);

  // Transaction statistics
  const totalTxs = reports.reduce((sum, r) => sum + (r.transactionHashes?.length || 0), 0);
  console.log(`\nðŸ“ Transaction Statistics:`);
  console.log(`   Total Transactions: ${totalTxs}`);
  console.log(`   Average per Test: ${(totalTxs / totalTests).toFixed(1)}`);

  // Test results breakdown
  console.log(`\nðŸ“‹ Test Results Breakdown:`);
  reports.forEach((report, index) => {
    const status = report.success ? "âœ… PASS" : "âŒ FAIL";
    const testName = path.basename(report.testName || `Test ${index + 1}`);
    const discrepancyCount = report.discrepancies?.length || 0;
    
    console.log(`   ${status} ${testName} (${report.executionTime.toFixed(1)}s, ${discrepancyCount} discrepancies)`);
  });

  // Failed test details
  if (failedTests > 0) {
    console.log(`\nâŒ Failed Test Details:`);
    reports
      .filter(r => !r.success)
      .forEach((report, index) => {
        const testName = path.basename(report.testName || `Failed Test ${index + 1}`);
        console.log(`\n   ${testName}:`);
        
        if (report.discrepancies && report.discrepancies.length > 0) {
          console.log(`     Discrepancies: ${report.discrepancies.length}`);
          report.discrepancies.slice(0, 3).forEach((disc: any) => {
            console.log(`       - ${disc.type} ${disc.entity || ""} ${disc.token}: expected ${disc.expected}, got ${disc.actual}`);
          });
          
          if (report.discrepancies.length > 3) {
            console.log(`       ... and ${report.discrepancies.length - 3} more discrepancies`);
          }
        }
      });
  }

  // Recommendations
  console.log(`\nðŸ’¡ Recommendations:`);
  if (failedTests === 0) {
    console.log(`   ðŸŽ‰ All tests passed! The rewards calculation system is working correctly.`);
  } else {
    console.log(`   ðŸ” Review failed tests for discrepancies in rewards calculations`);
    console.log(`   ðŸ“Š Check if discrepancies are within acceptable tolerance levels`);
    console.log(`   ðŸ› Investigate high discrepancy percentages for potential bugs`);
    
    if (avgTime > 60) {
      console.log(`   âš¡ Consider optimizing test execution time (currently ${avgTime.toFixed(1)}s average)`);
    }
  }

  console.log(`\nðŸ“ Detailed reports saved in: ${path.join(__dirname, "..", "reports")}`);
  console.log(`${"=".repeat(100)}\n`);
}

// Run if this script is executed directly
if (require.main === module) {
  main().catch(error => {
    console.error("Unhandled error in batch tests:", error);
    process.exit(1);
  });
}